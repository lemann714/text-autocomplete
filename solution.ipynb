{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d16defaf",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "372a5126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dotsenko.a.v/yandex/sprint2-project/sp2proj/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random, os, math, itertools, re, unicodedata\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    load_from_disk,\n",
    "    DatasetDict,\n",
    "    Dataset as HFDataset,\n",
    "    concatenate_datasets,          \n",
    ")\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup, AutoModelForCausalLM\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68026a36",
   "metadata": {},
   "source": [
    "# Глобальные переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dfc82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "CSV_PATH = './data/training.1600000.processed.noemoticon.csv' #\"./data/training.1600000.processed.noemoticon.csv\"\n",
    "ROUGE = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "URL_RE      = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n",
    "MENTION_RE  = re.compile(r'@\\w+')\n",
    "HASHTAG_RE  = re.compile(r'#(\\w+)')\n",
    "NUM_WORKERS = 0\n",
    "COMPARISON_DS_SIZE = 100 # количество записей из датасета для сравнения трансформера и lstm\n",
    "MAX_GEN_LEN = 20\n",
    "BEST_MODEL_PATH = f\"./models/best_models/final_model.pt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb42c950",
   "metadata": {},
   "source": [
    "# Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(csv_path: str,\n",
    "                 tokenizer: AutoTokenizer,\n",
    "                 max_length: int = 128,\n",
    "                 nrows: int | None = None) -> HFDataset:\n",
    "    \"\"\"\n",
    "    Возвращает HuggingFace-Dataset с колонками:\n",
    "        - input_ids (list[int])\n",
    "        - attention_mask (list[int])\n",
    "    \"\"\"\n",
    "    col_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "    df_raw = pd.read_csv(\n",
    "        csv_path,\n",
    "        header=None,\n",
    "        names=col_names,\n",
    "        encoding=\"latin1\",\n",
    "        dtype=str,\n",
    "        usecols=[\"text\"],\n",
    "        na_values=[\"NO_QUERY\"],\n",
    "        keep_default_na=False,\n",
    "        on_bad_lines=\"skip\",\n",
    "        nrows=nrows,\n",
    "    )\n",
    "    df_raw[\"text\"] = df_raw[\"text\"].apply(clean_tweet)\n",
    "    cleaned_texts = df_raw[\"text\"].tolist()\n",
    "    dataset = HFDataset.from_pandas(df_raw)\n",
    "    tokenized = dataset.map(\n",
    "        lambda batch: tokenizer(batch[\"text\"],\n",
    "                               truncation=True,\n",
    "                               max_length=max_length,\n",
    "                               padding=False,\n",
    "                               return_attention_mask=True),\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"]\n",
    "    )\n",
    "    return tokenized, cleaned_texts\n",
    "\n",
    "def split_hf_dataset(hf_dataset: HFDataset,\n",
    "                     train_ratio: float = 0.80,\n",
    "                     val_ratio:   float = 0.10,\n",
    "                     test_ratio:  float = 0.10,\n",
    "                     seed: int = 42) -> DatasetDict:\n",
    "    '''\n",
    "    Принимает токенизированный Dataset и возвращает\n",
    "    три датасета:\n",
    "        - train\n",
    "        - validation\n",
    "        - test\n",
    "\n",
    "    Параметры:\n",
    "        train_ratio, val_ratio, test_ratio – доли.\n",
    "        seed – фиксирует случайность, чтобы результаты были воспроизводимы.\n",
    "    '''\n",
    "    if not 0 < train_ratio < 1 or not 0 < val_ratio < 1 or not 0 < test_ratio < 1:\n",
    "        raise ValueError(\"Все доли должны лежать в (0,1)\")\n",
    "    split1 = hf_dataset.train_test_split(test_size=1.0 - train_ratio, seed=seed)\n",
    "    train_ds = split1['train']\n",
    "    rest = split1['test']\n",
    "\n",
    "    val_rel = val_ratio / (val_ratio + test_ratio)\n",
    "    split2 = rest.train_test_split(test_size=1.0 - val_rel, seed=seed)\n",
    "\n",
    "    val_ds = split2['train']\n",
    "    test_ds = split2['test']\n",
    "\n",
    "    return DatasetDict({\n",
    "        'train': train_ds,\n",
    "        'validation': val_ds,\n",
    "        'test': test_ds\n",
    "    })\n",
    "\n",
    "\n",
    "def remove_emoji(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Удаляет все символы, у которых Unicode-категория \n",
    "    начинается с 'S' (Symbol) или 'C' (Other, в т.ч. \n",
    "    контрольные символы).\n",
    "    \"\"\"\n",
    "    return ''.join(ch for ch in text\n",
    "                   if not (unicodedata.category(ch).startswith('S') or\n",
    "                           unicodedata.category(ch).startswith('C')))\n",
    "\n",
    "def clean_tweet(text_as_is: str) -> str:\n",
    "    \"\"\"\n",
    "    text_as_is - твит\n",
    "    \n",
    "    Убирает:\n",
    "        ссылки\n",
    "        упоминания\n",
    "        эмодзи\n",
    "        лишние пробелы\n",
    "\n",
    "    Возвращает: очищенный твит\n",
    "    \"\"\"\n",
    "    if not isinstance(text_as_is, str):\n",
    "        return ''\n",
    "\n",
    "    text = text_as_is.lower()\n",
    "    text = URL_RE.sub('', text)\n",
    "    text = MENTION_RE.sub('', text)\n",
    "    text = HASHTAG_RE.sub(r'\\1', text)\n",
    "    text = remove_emoji(text) \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def make_val_loader(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    eos_id: int,\n",
    "    device: torch.device,\n",
    "    nrows: int | None = None,\n",
    ") -> Tuple[DataLoader, DatasetDict, List[str]]:\n",
    "    \n",
    "    VAL_BATCH_SIZE = 10 # размер батча для сравнения трансформера и lstm\n",
    "    \"\"\"\n",
    "    Читает датасет, делит его на splits и возвращает\n",
    "    val_loader – DataLoader, готовый к использованию LSTM-моделью;\n",
    "    splits – словарь с HF-сплитами (можно взять validation-датасет);\n",
    "    val_texts – список чистых твитов (строк), которые нужны трансформеру.\n",
    "    \"\"\"\n",
    "    hf_tokenized, cleaned_texts = read_dataset(CSV_PATH,\n",
    "                                               tokenizer, \n",
    "                                               max_length=tokenizer.model_max_length,\n",
    "                                               nrows=nrows)\n",
    "    val_ds = TweetDataset(hf_tokenized, eos_id=eos_id)\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=VAL_BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=token_collate_fn,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=device.type == \"cuda\",\n",
    "    )\n",
    "    val_indices = hf_tokenized[\"__index__\"] if \"__index__\" in hf_tokenized.features else None\n",
    "    if val_indices is not None:\n",
    "        # Если у HF-датасета есть поле __index__, используем его.\n",
    "        val_texts = [cleaned_texts[i] for i in val_indices]\n",
    "    else:\n",
    "        # Если индексов нет (в старых версиях), просто берём первые N\n",
    "        # элементов, где N = len(splits[\"validation\"]).\n",
    "        val_texts = cleaned_texts[:len(hf_tokenized)]\n",
    "    return val_loader, hf_tokenized, val_texts, \n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, hf_dataset: HFDataset, eos_id: int):\n",
    "        self.input_ids = [ex[\"input_ids\"] for ex in hf_dataset]\n",
    "        self.attn_mask = [ex[\"attention_mask\"] for ex in hf_dataset]\n",
    "        self.eos_id = eos_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "                torch.tensor(self.attn_mask[idx], dtype=torch.long))\n",
    "    \n",
    "\n",
    "def token_collate_fn(batch):\n",
    "    # batch = [(ids, mask), …]\n",
    "    ids, masks = zip(*batch)\n",
    "    ids = pad_sequence(ids, batch_first=True, padding_value=0)\n",
    "    masks = pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "    # Для LSTM нам нужны также `labels` – сдвинутые на 1 токен\n",
    "    labels = ids.clone()\n",
    "    labels[:, :-1] = ids[:, 1:]\n",
    "    labels[:, -1] = -100          # ignore last token\n",
    "    return {\"input_ids\": ids,\n",
    "            \"attention_mask\": masks,\n",
    "            \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bdd729",
   "metadata": {},
   "source": [
    "# Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6435003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eos_id(tokenizer: AutoTokenizer) -> int:\n",
    "    \"\"\"Возвращает id конца последовательности\"\"\"\n",
    "    if tokenizer.eos_token_id is not None:\n",
    "        return tokenizer.eos_token_id\n",
    "    if tokenizer.sep_token_id is not None:\n",
    "        return tokenizer.sep_token_id\n",
    "    raise ValueError(\"Tokenizer has neither eos_token nor sep_token.\")\n",
    "\n",
    "def rouge_l_f1(ref: str, hyp: str) -> float:\n",
    "    return ROUGE.score(ref, hyp)['rougeL'].fmeasure\n",
    "\n",
    "def top_p_filtering(logits: Tensor, top_p: float = 0.9) -> Tensor:\n",
    "    \"\"\"\n",
    "    Оставляем только те токены, чья суммарная вероятность (по убыванию) ≤ top_p.\n",
    "    Возвращаем logits, где остальные токены заменены на -inf.\n",
    "    \"\"\"\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    # Оставляем токены, пока кумулятивная вероятность ≤ top_p\n",
    "    sorted_indices_to_keep = cumulative_probs <= top_p\n",
    "    # гарантируем, что как минимум один токен остаётся (первый)\n",
    "    sorted_indices_to_keep[..., 0] = 1\n",
    "\n",
    "    # Маска: -inf для токенов, которые выкинули\n",
    "    mask = torch.full_like(logits, float(\"-inf\"))\n",
    "    mask.scatter_(\n",
    "        dim=-1,\n",
    "        index=sorted_indices,\n",
    "        src=sorted_logits.masked_fill(~sorted_indices_to_keep, float(\"-inf\")),\n",
    "    )\n",
    "    return mask\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    '''\n",
    "    Позволяет выполнять детерминированный запуск\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1d802e",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4de8beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWordGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Train: Embedding → (Bi)LSTM → Linear (vocab size)\n",
    "    Inference: Embedding → LSTM → Linear (vocab size)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 embed_dim: int = 256,\n",
    "                 hidden_dim: int = 512,\n",
    "                 num_layers: int = 2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        lstm_out_dim = hidden_dim\n",
    "        self.fc = nn.Linear(lstm_out_dim, vocab_size)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.Tensor,\n",
    "                attention_mask: torch.Tensor,\n",
    "                labels: torch.Tensor | None = None):\n",
    "        '''\n",
    "        Возвращает логиты и ошибку\n",
    "        '''\n",
    "        embeds = self.embedding(input_ids)               # (B, L, D)\n",
    "\n",
    "        # защита от нулевых длин\n",
    "        lengths = attention_mask.sum(dim=1).cpu()        # (B,)\n",
    "        # Если в батче есть полностью пустые примеры (length == 0),\n",
    "        # заменяем 0 на 1, чтобы pack_padded_sequence не падала.\n",
    "        if (lengths == 0).any():\n",
    "            lengths = lengths.clone()\n",
    "            lengths[lengths == 0] = 1\n",
    "\n",
    "        packed = pack_padded_sequence(\n",
    "            embeds, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "        logits = self.fc(out)\n",
    "\n",
    "        if labels is None:\n",
    "            return {'logits': logits}\n",
    "        else:\n",
    "            loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            return {'logits': logits, 'loss': loss}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_one_word(self, \n",
    "                            text_prompt: str, \n",
    "                            tokenizer: AutoTokenizer, \n",
    "                            eos_id: int | None = None) -> str:\n",
    "        \"\"\"\n",
    "        Генерирует одно слово после заданного текстового префикса\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        self.eval()\n",
    "\n",
    "        # Токенизируем текст и получаем IDs\n",
    "        inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(device)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs.get(\"attention_mask\", None)\n",
    "\n",
    "        # Выполняем прямой проход через модель\n",
    "        outputs = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[\"logits\"][:, -1, :]  # Берем последние токены\n",
    "\n",
    "        # Предсказываем следующий токен\n",
    "        next_token = torch.argmax(logits, dim=-1)[0].item()\n",
    "\n",
    "        # Преобразуем токен обратно в текст\n",
    "        word = tokenizer.decode([next_token])\n",
    "        return word\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_n_words(self, \n",
    "                            text_prompt: str, \n",
    "                            n: int, \n",
    "                            tokenizer: AutoTokenizer, \n",
    "                            eos_id: int | None = None, \n",
    "                            do_sampling: bool = False, \n",
    "                            temperature: float = 1.0, \n",
    "                            top_p: float = 0.9) -> str:\n",
    "        \"\"\"\n",
    "        Генерирует N новых слов после заданного текстового префикса.\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        self.eval()\n",
    "\n",
    "        # Токенизируем входной текст\n",
    "        inputs = tokenizer(text_prompt, return_tensors=\"pt\").to(device)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs.get(\"attention_mask\", None)\n",
    "\n",
    "        # Начальные токены\n",
    "        current_ids = input_ids\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(n):\n",
    "                # Создаем внимание на всю длину текущих токенов\n",
    "                att_mask = torch.ones_like(current_ids, device=device)\n",
    "\n",
    "                # Прогоняем через модель\n",
    "                outputs = self.forward(input_ids=current_ids, attention_mask=att_mask)\n",
    "                logits = outputs[\"logits\"][:, -1, :]  # Последние токены\n",
    "\n",
    "                if do_sampling:\n",
    "                    # Применяем температуру\n",
    "                    logits /= max(temperature, 1e-8)\n",
    "\n",
    "                    # Отсекаем редкие токены по Top-P\n",
    "                    filtered_probs = F.softmax(top_p_filtering(logits.squeeze(), top_p=top_p), dim=-1)\n",
    "                    next_token = torch.multinomial(filtered_probs, num_samples=1).unsqueeze(0)\n",
    "                else:\n",
    "                    # Просто выбираем самый вероятный токен\n",
    "                    next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "                # Добавляем токен в последовательность\n",
    "                current_ids = torch.cat((current_ids, next_token), dim=1)\n",
    "\n",
    "                # Проверка на достижение символа конца строки\n",
    "                if eos_id is not None and next_token.item() == eos_id:\n",
    "                    break\n",
    "\n",
    "        # Переводим токены обратно в текст\n",
    "        result_text = tokenizer.decode(current_ids.squeeze().tolist())\n",
    "        return result_text[len(text_prompt):].strip()  # Убираем оригинальный текст и лишние пробелы\n",
    "\n",
    "    def generate_one_sample(self,\n",
    "                            prompt_ids: List[int],\n",
    "                            eos_id: int,\n",
    "                            num) -> List[int]:\n",
    "        \"\"\"\n",
    "        Жадная генерация (по-умолчанию).  \n",
    "        Останавливается, когда сгенерирован `eos_id` или\n",
    "        достигнут `max_gen_len`.\n",
    "\n",
    "        prompt_ids - Список токенов-промпта (может уже содержать `eos_id`).\n",
    "        eos_id - ID токена конца предложения.\n",
    "        к уже существующему промпту.\n",
    "\n",
    "        Возвращает Полный список токенов (промпт+сгенерированное продолжение).\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        self.eval()\n",
    "\n",
    "        # Защита от пустого промпта (аналогично generate)\n",
    "        if not prompt_ids:\n",
    "            return [] if eos_id is None else [eos_id]\n",
    "\n",
    "        # Приводим промпт к тензору формы (1, L)\n",
    "        generated = torch.tensor(prompt_ids,\n",
    "                                 dtype=torch.long,\n",
    "                                 device=device).unsqueeze(0)   # (1, L)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(MAX_GEN_LEN):\n",
    "                # учитывает все уже сгенерированные токены\n",
    "                attn_mask = torch.ones_like(generated, device=device)\n",
    "\n",
    "                out = self(input_ids=generated,\n",
    "                           attention_mask=attn_mask)          # logits: (1, cur_len, vocab)\n",
    "                next_token_logits = out['logits'][:, -1, :]      # (1, vocab)\n",
    "\n",
    "                # Жадный выбор (можно заменить на sampling/beam-search)\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1)  # (1)\n",
    "\n",
    "                # Добавляем выбранный токен к последовательности\n",
    "                generated = torch.cat([generated,\n",
    "                                      next_token.unsqueeze(-1)], dim=1)\n",
    "\n",
    "                # Прерываем, если получили EOS\n",
    "                if eos_id is not None and next_token.item() == eos_id:\n",
    "                    break\n",
    "\n",
    "        # Возвращаем чистый список int-ов (без batch-измерения)\n",
    "        return generated.squeeze().tolist()        \n",
    "    \n",
    "    def tokens_to_words(self, \n",
    "                        gen_ids: List[int],\n",
    "                        eos_id: int,\n",
    "                        tokenizer: AutoTokenizer) -> List[str]:\n",
    "        '''\n",
    "        Получает сгенерированные айдишнки\n",
    "        Берет полезную часть (до паддинга)\n",
    "        На выходе декодированный текст\n",
    "        '''\n",
    "        # Обрезаем сгенерированный токен-список до EOS (если он есть) ----\n",
    "        if eos_id in gen_ids:\n",
    "            gen_ids = gen_ids[:gen_ids.index(eos_id)]\n",
    "\n",
    "        # Декодируем оба текста ----\n",
    "        gen_words = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "        return gen_words\n",
    "    \n",
    "    def words_to_tokens(\n",
    "        self,\n",
    "        tweet: str,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        add_special_tokens: bool = False) -> List[str]:\n",
    "        \"\"\"\n",
    "        Преобразует один твит в список токенов.\n",
    "        \"\"\"\n",
    "        enc = tokenizer(tweet,\n",
    "                         truncation=True,\n",
    "                         max_length=MAX_GEN_LEN,\n",
    "                         add_special_tokens=add_special_tokens,\n",
    "                         padding=False)\n",
    "        prompt_ids = [ids for ids in enc['input_ids']]\n",
    "        return prompt_ids\n",
    "\n",
    "    def complete(self, *, \n",
    "                 text: str,\n",
    "                 eos_id: int,\n",
    "                 tokenizer: AutoTokenizer,\n",
    "                 add_special_tokens=False,\n",
    "                 preprocess: bool = True) -> str:\n",
    "        \n",
    "        prompt_ids = self.words_to_tokens(text, tokenizer, add_special_tokens, MAX_GEN_LEN)\n",
    "        generated_ids = self.generate(prompt_ids=prompt_ids,\n",
    "                                      eos_id=eos_id,\n",
    "                                      do_sampling=True,\n",
    "                                      temperature=0.7,\n",
    "                                      top_p=0.8)\n",
    "        generated_text = self.tokens_to_words(generated_ids, eos_id, tokenizer)\n",
    "        return generated_text\n",
    "\n",
    "    def generate(self,\n",
    "                prompt_ids: List[int],\n",
    "                eos_id: int,\n",
    "                do_sampling: bool = False,\n",
    "                temperature: float = 1.0,\n",
    "                top_p: float = 0.9) -> List[int]:\n",
    "        \"\"\"\n",
    "        Универсальный генератор, поддерживает:\n",
    "        * greedy (do_sampling=False)\n",
    "        * sampling с temperature / top-p (do_sampling=True)\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        self.eval()\n",
    "\n",
    "        # Защита от полностью пустого промпта\n",
    "        if not prompt_ids:                     # ничего не генерируем\n",
    "            # Возвращаем либо пустой список, либо [eos_id] – выбираем\n",
    "            # вариант, который проще обрабатывается дальше.\n",
    "            return [] if eos_id is None else [eos_id]\n",
    "\n",
    "        # (1, L) – уже tokenы промпта\n",
    "        generated = torch.tensor(prompt_ids,\n",
    "                                 dtype=torch.long,\n",
    "                                 device=device).unsqueeze(0)   # (1, L)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(MAX_GEN_LEN):\n",
    "                attn_mask = torch.ones_like(generated, device=device)\n",
    "\n",
    "                out = self(input_ids=generated,\n",
    "                           attention_mask=attn_mask)  # logits (1, cur_len, vocab)\n",
    "                next_logits = out[\"logits\"][:, -1, :]                     # (1, vocab)\n",
    "\n",
    "                if do_sampling:\n",
    "                    # temperature\n",
    "                    logits = next_logits / max(temperature, 1e-8)\n",
    "\n",
    "                    # top-p\n",
    "                    logits = top_p_filtering(logits.squeeze(0), top_p=top_p).unsqueeze(0)\n",
    "\n",
    "                    # выбор из распределения\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)   # (1,1)\n",
    "                else:            # greedy\n",
    "                    next_token = torch.argmax(next_logits, dim=-1, keepdim=True)  # (1,1)\n",
    "\n",
    "                generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "                # EOS?\n",
    "                if eos_id is not None and next_token.item() == eos_id:\n",
    "                    break\n",
    "\n",
    "        return generated.squeeze().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec6c936",
   "metadata": {},
   "source": [
    "# Сетап обучения LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019e5dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(\n",
    "    prompt_ids: List[int],\n",
    "    model: AutoModelForCausalLM,\n",
    "    eos_id: int,\n",
    "    do_sampling: bool = False,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.9,\n",
    ") -> List[int]:\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = torch.tensor([prompt_ids], dtype=torch.long, device=device)\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": MAX_GEN_LEN,\n",
    "        \"eos_token_id\": eos_id,\n",
    "        \"do_sample\": do_sampling,\n",
    "    }\n",
    "    if do_sampling:\n",
    "        gen_kwargs.update(\n",
    "            {\"temperature\": temperature, \"top_p\": top_p}\n",
    "        )\n",
    "    out = model.generate(input_ids, **gen_kwargs)          # (1, prompt+generated)\n",
    "    generated = out[0][input_ids.shape[-1] :].tolist()    # только сгенерированное\n",
    "    return generated\n",
    "\n",
    "def evaluate_on_loader(model: nn.Module,\n",
    "                        loader: DataLoader,\n",
    "                        tokenizer: AutoTokenizer,\n",
    "                        device: torch.device,\n",
    "                        eos_id: int,\n",
    "                        fraction: float = 0.5,\n",
    "                        use_sampling: bool = False,\n",
    "                        temperature: float = 0.7,\n",
    "                        top_p: float = 0.9,\n",
    "                        verbose: bool = False) -> Tuple[float, float, List[float] | None]:\n",
    "    \"\"\"\n",
    "    Возвращает (perplexity, avg_rougeL, per_example_scores|None)\n",
    "    Оценка одинаково работает для LSTM и для трансформера,\n",
    "    при условии, что модель реализует метод `generate_one_sample`\n",
    "    (у LSTM он уже есть, у трансформера реализуем «обёртку» ниже).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_nll, total_tokens = 0.0, 0\n",
    "    rouge_sum, n_examples = 0.0, 0\n",
    "    per_example = [] if verbose else None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # perplexity\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attn_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels    = batch[\"labels\"].to(device)\n",
    "\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attn_mask,\n",
    "                        labels=labels)\n",
    "            loss = out[\"loss\"]\n",
    "            # количество токенов без паддинга:\n",
    "            n_tok = attn_mask.sum().item()\n",
    "            total_nll += loss.item() * n_tok\n",
    "            total_tokens += n_tok\n",
    "\n",
    "            #  ROUGE \n",
    "            ids_np = input_ids.cpu().numpy()\n",
    "            for i in range(ids_np.shape[0]):\n",
    "                seq = ids_np[i].tolist()\n",
    "                # убираем PAD и EOS\n",
    "                if 0 in seq:\n",
    "                    seq = seq[:seq.index(0)]\n",
    "                if eos_id in seq:\n",
    "                    seq = seq[:seq.index(eos_id)]\n",
    "\n",
    "                if not seq:\n",
    "                    continue\n",
    "\n",
    "                split = int(len(seq) * fraction)\n",
    "                prompt_ids = seq[:split]\n",
    "                ref_ids    = seq[split:]\n",
    "\n",
    "                # генерация (можно использовать единый интерфейс)\n",
    "                if isinstance(model, LSTMWordGenerator):\n",
    "                    gen_ids = model.generate(prompt_ids=prompt_ids,\n",
    "                                            eos_id=eos_id,\n",
    "                                            do_sampling=use_sampling,\n",
    "                                            temperature=temperature,\n",
    "                                            top_p=top_p)\n",
    "                else:   # трансформер\n",
    "                    gen_ids = complete_text(prompt_ids=prompt_ids,\n",
    "                                            tokenizer=tokenizer,\n",
    "                                            model=model,\n",
    "                                            eos_id=eos_id,\n",
    "                                            do_sampling=use_sampling,\n",
    "                                            temperature=temperature,\n",
    "                                            top_p=top_p)\n",
    "                # декодируем\n",
    "                gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "                ref_text = tokenizer.decode(ref_ids, skip_special_tokens=True)\n",
    "\n",
    "                rouge_f = rouge_l_f1(ref_text, gen_text)\n",
    "                rouge_sum += rouge_f\n",
    "                n_examples += 1\n",
    "                if verbose:\n",
    "                    per_example.append(rouge_f)\n",
    "\n",
    "    perplexity = math.exp(total_nll / total_tokens) if total_tokens > 0 else float('inf')\n",
    "    avg_rouge = rouge_sum / n_examples if n_examples > 0 else 0.0\n",
    "    return perplexity, avg_rouge, per_example\n",
    "\n",
    "def grid_search(\n",
    "    param_grid: Dict[str, List[Any]],\n",
    "    base_train_loader: DataLoader,\n",
    "    base_val_loader: DataLoader,\n",
    "    base_test_loader: DataLoader,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    device: torch.device,\n",
    "    eos_id: int,\n",
    "    results_dir: str = \"models\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Перебирает все комбинации параметров, обучает модель,\n",
    "    сохраняет лучшую модель и графики, возвращает таблицу-результатов.\n",
    "    \"\"\"\n",
    "    print('\\nИщем оптимальные параметры обучения\\n')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(results_dir, \"best_models\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(results_dir, \"plots\"), exist_ok=True)\n",
    "\n",
    "    # список всех вариантов в виде dict\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combos = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    all_records = []\n",
    "\n",
    "    for idx, cfg in enumerate(combos, start=1):\n",
    "        print(f\"\\nРассмотри {idx}/{len(combos)} – cfg: {cfg}\\n\")\n",
    "\n",
    "        lr          = cfg[\"lr\"]\n",
    "        embed_dim   = cfg[\"embed_dim\"]\n",
    "        hidden_dim  = cfg[\"hidden_dim\"]\n",
    "        num_layers  = cfg[\"num_layers\"]\n",
    "        batch_size  = cfg[\"batch_size\"]\n",
    "        epochs      = cfg[\"epochs\"]\n",
    "\n",
    "        # пересоздаём даталоадеры, если меняется batch_size\n",
    "        train_loader = DataLoader(\n",
    "            base_train_loader.dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=token_collate_fn,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=device.type == \"cuda\",\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            base_val_loader.dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=token_collate_fn,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=device.type == \"cuda\",\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            base_test_loader.dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=token_collate_fn,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=device.type == \"cuda\",\n",
    "        )\n",
    "\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        model = LSTMWordGenerator(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "        ).to(device)\n",
    "\n",
    "        best_path = os.path.join(\n",
    "            results_dir, \"best_models\", f\"run_{idx:03d}_best.pt\"\n",
    "        )\n",
    "        model, history = train(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            tokenizer,\n",
    "            device,\n",
    "            eos_id,\n",
    "            epochs=epochs,\n",
    "            lr=lr,\n",
    "            warmup_ratio=0.0,         \n",
    "            patience=3,\n",
    "            best_model_path=best_path,\n",
    "        )\n",
    "\n",
    "        test_ppl, test_rouge, _ = evaluate_on_loader(\n",
    "            model,\n",
    "            test_loader,\n",
    "            tokenizer,\n",
    "            device,\n",
    "            eos_id,\n",
    "            fraction=0.5,\n",
    "        )\n",
    "        print(f\"Test PPL: {test_ppl:.2f} |  Test ROUGE-L: {test_rouge:.4f}\")\n",
    "\n",
    "       # train_loss + val_rougeL\n",
    "        fig1, ax1 = plt.subplots(figsize=(6, 4))\n",
    "        ax1.plot(\n",
    "            [h[\"epoch\"] for h in history],\n",
    "            [h[\"train_loss\"] for h in history],\n",
    "            label=\"train loss\",\n",
    "            marker=\"o\",\n",
    "            color=\"#1f77b4\",\n",
    "        )\n",
    "        ax1.plot(\n",
    "            [h[\"epoch\"] for h in history],\n",
    "            [h[\"val_rougeL\"] for h in history],\n",
    "            label=\"val ROUGE-L\",\n",
    "            marker=\"s\",\n",
    "            color=\"#ff7f0e\",\n",
    "        )\n",
    "        ax1.set_xlabel(\"epoch\")\n",
    "        ax1.set_ylabel(\"value\")\n",
    "        ax1.set_title(f\"Run {idx:03d} – train loss / val ROUGE-L\")\n",
    "        ax1.legend()\n",
    "        plt.tight_layout()\n",
    "        fig1.savefig(\n",
    "            os.path.join(results_dir, \"plots\", f\"run_{idx:03d}_loss_rouge.png\")\n",
    "        )\n",
    "        plt.close(fig1)\n",
    "\n",
    "        # train_loss + val_perplexity\n",
    "        fig2, ax2 = plt.subplots(figsize=(6, 4))\n",
    "        ax2.plot(\n",
    "            [h[\"epoch\"] for h in history],\n",
    "            [h[\"train_loss\"] for h in history],\n",
    "            label=\"train loss\",\n",
    "            marker=\"o\",\n",
    "            color=\"#1f77b4\",\n",
    "        )\n",
    "        ax2.plot(\n",
    "            [h[\"epoch\"] for h in history],\n",
    "            [h[\"val_perplexity\"] for h in history],\n",
    "            label=\"val perplexity\",\n",
    "            marker=\"x\",\n",
    "            color=\"#2ca02c\",\n",
    "        )\n",
    "        ax2.set_xlabel(\"epoch\")\n",
    "        ax2.set_ylabel(\"value\")\n",
    "        ax2.set_title(f\"Run {idx:03d} – train loss / val perplexity\")\n",
    "        ax2.legend()\n",
    "        plt.tight_layout()\n",
    "        fig2.savefig(\n",
    "            os.path.join(results_dir, \"plots\", f\"run_{idx:03d}_loss_ppl.png\")\n",
    "        )\n",
    "        plt.close(fig2)\n",
    "\n",
    "        # аписываем строку в итоговую таблицу\n",
    "        record = {\n",
    "            **cfg,\n",
    "            \"final_train_loss\": history[-1][\"train_loss\"],\n",
    "            \"final_val_perplexity\": history[-1][\"val_perplexity\"],\n",
    "            \"final_val_rougeL\": history[-1][\"val_rougeL\"],\n",
    "            \"test_perplexity\": test_ppl,\n",
    "            \"test_rougeL\": test_rouge,\n",
    "            \"best_model_path\": best_path,\n",
    "        }\n",
    "        all_records.append(record)\n",
    "\n",
    "        # сохраняем промежуточный CSV после каждой итерации (чтобы не потерять результаты)\n",
    "        pd.DataFrame(all_records).to_csv(\n",
    "            os.path.join(results_dir, \"grid_search_results.csv\"),\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "    #возврат полной таблицы\n",
    "    results_df = pd.DataFrame(all_records)\n",
    "    results_df.to_csv(\n",
    "        os.path.join(results_dir, \"grid_search_results.csv\"),\n",
    "        index=False,\n",
    "    )\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def train(model: LSTMWordGenerator,\n",
    "          train_loader: DataLoader,\n",
    "          val_loader: DataLoader,\n",
    "          tokenizer: AutoTokenizer,\n",
    "          device: torch.device,\n",
    "          eos_id: int,\n",
    "          epochs: int = 10,\n",
    "          lr: float = 5e-4,\n",
    "          warmup_ratio: float = 0.1,\n",
    "          patience: int = 2,\n",
    "          best_model_path: str = 'best_blstm.pt') -> LSTMWordGenerator:\n",
    "    '''\n",
    "    Собирает историю метрик (train_loss, val_ppl, val_rouge) для каждой эпохи.\n",
    "\n",
    "    Возвращает обученную модель (с лучшими весами) и список словарей-записей.\n",
    "    '''\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    # warmup_steps = int(warmup_ratio * total_steps)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # scheduler = get_linear_schedule_with_warmup(\n",
    "    #     optimizer,\n",
    "    #     num_training_steps=total_steps,\n",
    "    #     num_warmup_steps=warmup_steps\n",
    "    # )\n",
    "\n",
    "    best_val_rouge = -float('inf')\n",
    "    no_improve = 0\n",
    "    history = []\n",
    "\n",
    "    # train\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(input_ids=batch['input_ids'].to(device),\n",
    "                        attention_mask=batch['attention_mask'].to(device),\n",
    "                        labels=batch['labels'].to(device))\n",
    "            \n",
    "            loss = out['loss']\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item() * batch['input_ids'].size(0)\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # validation\n",
    "        val_ppl, val_rouge, _ = evaluate_on_loader(model,\n",
    "                                                val_loader,\n",
    "                                                tokenizer,\n",
    "                                                device,\n",
    "                                                eos_id)\n",
    "        print(f'\\nEpoch {epoch:02d} | train_loss={avg_train_loss:.4f}'\n",
    "              f' | valid. ppl={val_ppl:.2f} | valid.ROUGE-L={val_rouge:.4f}')\n",
    "        history.append(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_perplexity\": val_ppl,\n",
    "                \"val_rougeL\": val_rouge,\n",
    "            }\n",
    "        )\n",
    "        # early stopping\n",
    "        if val_rouge > best_val_rouge:\n",
    "            best_val_rouge = val_rouge\n",
    "            no_improve = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                break\n",
    "\n",
    "    # загрузим лучшую\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def train_final(best_cfg: dict,\n",
    "                splits: DatasetDict,\n",
    "                tokenizer: AutoTokenizer,\n",
    "                device: torch.device,\n",
    "                eos_id: int,\n",
    "                final_model_path: str = 'full_final_model.pt',\n",
    "                results_dir: str = 'models'):\n",
    "    '''\n",
    "    Объединяет train, val, test. обучает модель с лучшими гиперпараметрами\n",
    "    и сохраняет её в <results_dir>/final_model.pt.\n",
    "    Возвращает обученную модель.\n",
    "    '''\n",
    "    print('\\nОбучение финальной модели...\\n')\n",
    "    full_dataset = concatenate_datasets([splits[\"train\"],\n",
    "                                         splits[\"validation\"],\n",
    "                                         splits[\"test\"]])\n",
    "\n",
    "    full_tweet_ds = TweetDataset(full_dataset, eos_id=eos_id)\n",
    "    batch_size = best_cfg[\"batch_size\"]\n",
    "    # Даталоадер (только train-loader, валидации нет)\n",
    "    train_loader = DataLoader(\n",
    "        full_tweet_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=token_collate_fn,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=device.type == \"cuda\",\n",
    "    )\n",
    "    # Инициализируем модель с найденными гиперпараметрами\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    model = LSTMWordGenerator(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=best_cfg[\"embed_dim\"],\n",
    "        hidden_dim=best_cfg[\"hidden_dim\"],\n",
    "        num_layers=best_cfg[\"num_layers\"]).to(device)\n",
    "\n",
    "    # обучаем с использованием early-stopping для защиты от переобучения\n",
    "    # но теперь мониторим ROUGE-L на 10% от полной выборки\n",
    "    split_tmp = full_dataset.train_test_split(test_size=0.10, seed=42)\n",
    "    val_hf = split_tmp[\"test\"]\n",
    "    val_ds = TweetDataset(val_hf, eos_id=eos_id)\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=token_collate_fn,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=device.type == \"cuda\",\n",
    "    )\n",
    "\n",
    "    epochs = best_cfg[\"epochs\"] * 2 # удваиваем чтобы дать модели шанс разойтись на большом объёме, но early-stop всё равно прервет\n",
    "    lr = best_cfg[\"lr\"]\n",
    "    patience = 3\n",
    "\n",
    "    model, _ = train(\n",
    "        model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        eos_id=eos_id,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        warmup_ratio=0.0,         \n",
    "        patience=patience,\n",
    "        best_model_path=os.path.join(results_dir, \"best_models/final_model.pt\")\n",
    "    )\n",
    "\n",
    "    torch.save(model, os.path.join(results_dir, f\"best_models/{final_model_path}\"))\n",
    "\n",
    "    print(f\"\\nВеса итоговой модели сохранены в {os.path.join(results_dir, 'final_model.pt')}\")\n",
    "    print(f\"\\nПолная финальная модель сохранена в {os.path.join(results_dir, 'full_final_model.pt')}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_lstm(tokenizer: AutoTokenizer,\n",
    "               eos_id: int):\n",
    "    # Поиск оптимальных параметров обучения\n",
    "    grid = {\n",
    "        \"lr\":          [5e-4],\n",
    "        \"embed_dim\":   [128],\n",
    "        \"hidden_dim\":  [128],\n",
    "        \"num_layers\":  [1],\n",
    "        \"batch_size\":  [16],            \n",
    "        \"epochs\":      [4],\n",
    "    }\n",
    "\n",
    "    # общие настройки ----------\n",
    "    set_seed(2025)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Чтение и токенизация ----------\n",
    "    # MAX_LEN  = 128\n",
    "    NROWS    = None if device.type == \"cuda\" else 50              \n",
    "    hf, cleaned_texts = read_dataset(CSV_PATH, \n",
    "                                     tokenizer, \n",
    "                                     max_length=tokenizer.model_max_length, \n",
    "                                     nrows=NROWS)\n",
    "\n",
    "    # делим на сплиты ----------\n",
    "    splits = split_hf_dataset(\n",
    "        hf, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=2025\n",
    "    )\n",
    "\n",
    "    # даталоадеры будут пересозданы внутри grid_search\n",
    "    # они нужны только чтобы взять `dataset` и параметры `num_workers/pin_memory`\n",
    "    base_train_ds = TweetDataset(splits[\"train\"], eos_id=eos_id) \n",
    "    base_val_ds   = TweetDataset(splits[\"validation\"], eos_id=eos_id)\n",
    "    base_test_ds  = TweetDataset(splits[\"test\"], eos_id=eos_id)\n",
    "    base_train_loader = DataLoader(\n",
    "        base_train_ds,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        collate_fn=token_collate_fn,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=device.type == \"cuda\",\n",
    "    )\n",
    "    base_val_loader = DataLoader(\n",
    "        base_val_ds,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        collate_fn=token_collate_fn,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=device.type == \"cuda\",\n",
    "    )\n",
    "    base_test_loader = DataLoader(\n",
    "        base_test_ds,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        collate_fn=token_collate_fn,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=device.type == \"cuda\",\n",
    "    )\n",
    "\n",
    "    # Запуск grid-search\n",
    "    results_df = grid_search(\n",
    "        param_grid=grid,\n",
    "        base_train_loader=base_train_loader,\n",
    "        base_val_loader=base_val_loader,\n",
    "        base_test_loader=base_test_loader,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        eos_id=eos_id,\n",
    "        results_dir=\"models\",\n",
    "    )\n",
    "\n",
    "    # Находим лучшую конфигурацию (по тестовому ROUGE-L)\n",
    "    best_cfg = results_df.loc[results_df[\"test_rougeL\"].idxmax()].to_dict()\n",
    "    print(\"\\nЛучшая модель (по test-rougeL)\")\n",
    "    print(best_cfg)\n",
    "    \n",
    "    # Обучение финальной модели (без сплита данных)\n",
    "    final_model = train_final(best_cfg=best_cfg,\n",
    "                              splits=splits,\n",
    "                              tokenizer=tokenizer,\n",
    "                              device=device,\n",
    "                              eos_id=eos_id,\n",
    "                              results_dir='models')\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96745bf",
   "metadata": {},
   "source": [
    "# Сетап инференса LSTM & Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b47ef995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_lstm(*,\n",
    "                   tokenizer: AutoTokenizer,\n",
    "                   eos_id: int,\n",
    "                   use_sampling: bool = False,\n",
    "                   temperature: float = 1.0,\n",
    "                   top_p: float = 0.9):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # best_lstm_path = \"./models/best_models/full_final_model.pt\" \n",
    "    # model = torch.load(best_lstm_path, map_location=device) # полная модель не хочет загружаться:\n",
    "    '''\n",
    "    AttributeError: Can't get attribute 'LSTMWordGenerator' \n",
    "    on <module '__main__' from '/Users/dotsenko.a.v/yandex/sprint2-project/src/compare.py'>\n",
    "    '''\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    model = LSTMWordGenerator(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=128,\n",
    "        hidden_dim=128,\n",
    "        num_layers=1).to(device)\n",
    "    model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Формируем DataLoader для validation-части.\n",
    "    val_loader, hf, val_texts = make_val_loader(tokenizer=tokenizer,\n",
    "                                                    eos_id=eos_id,\n",
    "                                                    device=device,\n",
    "                                                    nrows=COMPARISON_DS_SIZE)\n",
    "\n",
    "    # Вычисляем метрики. verbose=True заставит функцию вернуть\n",
    "    #      список ROUGE-L по каждому примеру.\n",
    "    ppl, avg_rouge, per_example = evaluate_on_loader(model=model,\n",
    "                                                        loader=val_loader,\n",
    "                                                        tokenizer=tokenizer,\n",
    "                                                        device=device,\n",
    "                                                        eos_id=eos_id,\n",
    "                                                        fraction=0.5,\n",
    "                                                        verbose=True,\n",
    "                                                        use_sampling=use_sampling,\n",
    "                                                        temperature=temperature,\n",
    "                                                        top_p=top_p)\n",
    "\n",
    "    val_hf = hf\n",
    "    autocompleted_texts = []\n",
    "    for i in range(len(val_hf)):\n",
    "        ids = val_hf[i][\"input_ids\"]\n",
    "        # убираем PAD (0) и EOS, если они есть\n",
    "        if 0 in ids:\n",
    "            ids = ids[:ids.index(0)]\n",
    "        if eos_id in ids:\n",
    "            ids = ids[:ids.index(eos_id)]\n",
    "        # получаем чистый текст твита\n",
    "        src_text = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        # генерируем продолжение (используем уже обученную модель)\n",
    "        fraction = 0.5\n",
    "        split_idx = int(len(src_text) * fraction)\n",
    "        prompt_text = src_text[:split_idx] \n",
    "        compl = model.generate(\n",
    "            prompt_ids=tokenizer.encode(prompt_text, add_special_tokens=False),\n",
    "            eos_id=eos_id,\n",
    "            do_sampling=use_sampling,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        compl_text = tokenizer.decode(compl, skip_special_tokens=True)\n",
    "        autocompleted_texts.append({'prompt': prompt_text, \n",
    "                                    'prediction': compl_text})\n",
    "\n",
    "    return avg_rouge, autocompleted_texts\n",
    "\n",
    "\n",
    "def generate_completion(prompt: str,\n",
    "                        tokenizer: AutoTokenizer,\n",
    "                        model: AutoModelForCausalLM,\n",
    "                        temperature: float = 1.0,\n",
    "                        top_p: float = 0.9,\n",
    "                        do_sample: bool = False) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample,\n",
    "        )\n",
    "    generated = out_ids[0][inputs[\"input_ids\"].shape[-1] :]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def validate_on_dataset(texts: list[str],\n",
    "                        tokenizer: AutoTokenizer,\n",
    "                        model: AutoModelForCausalLM,\n",
    "                        fraction: float = 0.5,\n",
    "                        do_sampling: bool = False,\n",
    "                        temperature: float = 1.0,\n",
    "                        top_p: float = 0.9) -> dict:\n",
    "    \"\"\"\n",
    "    Для каждого текста берём первые 75 % как prompt,\n",
    "    а оставшиеся – как reference.\n",
    "    \"\"\"\n",
    "    f1_scores = []\n",
    "    examples = []\n",
    "\n",
    "    for txt in texts:\n",
    "        split_idx = int(len(txt) * fraction)\n",
    "        prompt, reference = txt[:split_idx], txt[split_idx:]\n",
    "\n",
    "        pred = generate_completion(\n",
    "            prompt,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            # switch between greedy / sampling\n",
    "            do_sample=do_sampling\n",
    "        )\n",
    "\n",
    "        score = rouge_l_f1(reference, pred)\n",
    "        f1_scores.append(score)\n",
    "\n",
    "        examples.append({\n",
    "            \"prompt\":      prompt,\n",
    "            \"prediction\":  pred\n",
    "        })\n",
    "\n",
    "    avg_f1 = float(np.mean(f1_scores)) if f1_scores else 0.0\n",
    "    return {\"rougeL_f1\": avg_f1, \"examples\": examples}\n",
    "\n",
    "def inference_transformer(*,\n",
    "                          transformer_name: str,\n",
    "                          tokenizer: AutoTokenizer,\n",
    "                          eos_id: int,\n",
    "                          use_sampling: bool = False,\n",
    "                          temperature: float = 1.0,\n",
    "                          top_p: float = 0.9):\n",
    "    model = AutoModelForCausalLM.from_pretrained(transformer_name)\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(device)\n",
    "\n",
    "    # Получаем готовый DataLoader и **тексты** для трансформера\n",
    "    val_loader, hf, val_texts = make_val_loader(\n",
    "        tokenizer=tokenizer,          \n",
    "        eos_id=eos_id,\n",
    "        device=device,\n",
    "        nrows=COMPARISON_DS_SIZE,                     \n",
    "    )\n",
    "\n",
    "    # Валидация трансформера (ROUGE-L F1)\n",
    "    results = validate_on_dataset(\n",
    "                texts=val_texts,\n",
    "                tokenizer=tokenizer,\n",
    "                model=model,\n",
    "                fraction=0.5,\n",
    "                # передаём параметры в generate()\n",
    "                do_sampling=use_sampling,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "    )\n",
    "    avg_rouge = results['rougeL_f1']\n",
    "    # autocompleted_texts = [{'prompt': ex['prompt'], \n",
    "    #                         'prediction': ex['prediction']} for i, ex in enumerate(results[\"examples\"], 1)]\n",
    "    autocompleted_texts = results[\"examples\"] \n",
    "    return avg_rouge, autocompleted_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea0b45",
   "metadata": {},
   "source": [
    "# Переходим к экспериментам: можно пропустить ячейку с обучением LSTM для проверки весов, полученных на VM. Потом для проверки работы обучения можно раскоментировать и запустить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb37856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# надо понять какой токенизатор у трансформера\n",
    "TRANSFORMER_NAME = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_NAME)\n",
    "eos_id = get_eos_id(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4edf9b",
   "metadata": {},
   "source": [
    "# Обучение LSTM (если раскоментировать train_lstm, то модель из репы перезатрется)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cf88780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучаем с этим токенизатором\n",
    "# model = train_lstm(tokenizer=tokenizer, eos_id=eos_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770af85",
   "metadata": {},
   "source": [
    "# Попытки автодополнения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4978c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM локанично сказала: Moscow is  sick\n",
      "LSTM разговорилась: Moscow is sick!!!!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = LSTMWordGenerator(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_layers=1).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "initial_text = \"Moscow is\"\n",
    "\n",
    "print(f'LSTM локанично сказала: {initial_text}', model.generate_one_word(initial_text, tokenizer))\n",
    "\n",
    "print(f'LSTM разговорилась: {initial_text}', model.generate_n_words(initial_text, n=5, tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f52c41",
   "metadata": {},
   "source": [
    "# Сравнение скорингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5aba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2604.30 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8074.67 examples/s]\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 7225.58 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 8491.18 examples/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Жадная генерация\n",
    "mean_rouge_lstm_greedy, lstm_greedy_texts = inference_lstm(\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    eos_id=eos_id,\n",
    "                                    use_sampling=False)\n",
    "\n",
    "mean_rouge_transformer_greedy, transformer_greedy_texts = inference_transformer(\n",
    "                                            transformer_name=TRANSFORMER_NAME,\n",
    "                                            tokenizer=tokenizer,\n",
    "                                            eos_id=eos_id,\n",
    "                                            use_sampling=False)\n",
    "\n",
    "\n",
    "temperature=0.8\n",
    "top_p=0.9\n",
    "mean_rouge_lstm_sampling, lstm_sampling_texts = inference_lstm(\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        eos_id=eos_id,\n",
    "                                        use_sampling=True,\n",
    "                                        temperature=temperature,\n",
    "                                        top_p=top_p)\n",
    "\n",
    "mean_rouge_transformer_sampling, transformer_sampling_texts = inference_transformer(\n",
    "                                            transformer_name=TRANSFORMER_NAME,\n",
    "                                            tokenizer=tokenizer,\n",
    "                                            eos_id=eos_id,\n",
    "                                            use_sampling=True,\n",
    "                                            temperature=temperature,\n",
    "                                            top_p=top_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba8934",
   "metadata": {},
   "source": [
    "# Интерпретация результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e837df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_rouge_lstm_greedy=0.07065358683050327\n",
      "mean_rouge_transformer_greedy=0.051920717352681785\n",
      "mean_rouge_lstm_sampling=0.06695747807311582\n",
      "mean_rouge_transformer_sampling=0.05895486075012723\n"
     ]
    }
   ],
   "source": [
    "print(f'{mean_rouge_lstm_greedy=}')\n",
    "print(f'{mean_rouge_transformer_greedy=}')\n",
    "print(f'{mean_rouge_lstm_sampling=}')\n",
    "print(f'{mean_rouge_transformer_sampling=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01aeb21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM с семплированием:\n",
      "PROMT: - awww, that's a bummer. you shoulda g\n",
      "PREDICTION: - awww, that's a bummer. you shoulda gals! but i'm tired!!!!!!!!!! no!!!\n",
      "\n",
      "PROMT: is upset that he can't update his facebook by texting \n",
      "PREDICTION: is upset that he can't update his facebook by texting ive a little!.!!, but not having a tummy ache!!!!\n",
      "\n",
      "PROMT: i dived many times for the ball. manage\n",
      "PREDICTION: i dived many times for the ball. manage to get the front-it's so was drunk!!!!! ...!!!s\n",
      "\n",
      "PROMT: my whole body feels itc\n",
      "PREDICTION: my whole body feels itc!!!! is only rude and i want to cry...!! the opportunity is wemble\n",
      "\n",
      "PROMT: no, it's not behaving at all. i'm mad. why am \n",
      "PREDICTION: no, it's not behaving at all. i'm mad. why am ive been procrastinating anymore! i hate that it has been worse than so!!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('LSTM с семплированием:')\n",
    "for j in range(5):\n",
    "    print(f'PROMT: {lstm_sampling_texts[j][\"prompt\"]}')\n",
    "    print(f'PREDICTION: {lstm_sampling_texts[j][\"prediction\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2edeca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMER с семплированием:\n",
      "PROMT: - awww, that's a bummer. you shoulda g\n",
      "PREDICTION: iddy if it's a bit of a surprise. The bummer is that it's the third time\n",
      "\n",
      "PROMT: is upset that he can't update his facebook by texting i\n",
      "PREDICTION:  had to do it and after I have seen that he is not posting anything to his friends. I\n",
      "\n",
      "PROMT: i dived many times for the ball. manage\n",
      "PREDICTION:  to stay in the back of the net.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROMT: my whole body feels itc\n",
      "PREDICTION: \n",
      "\n",
      "The thing is, I feel very, very happy with this body. I feel pretty good\n",
      "\n",
      "PROMT: no, it's not behaving at all. i'm mad. why am \n",
      "PREDICTION: icky? i'm not mad. why am icky? i'm not mad. why am \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('TRANSFORMER с семплированием:')\n",
    "for j in range(5):\n",
    "    print(f'PROMT: {transformer_sampling_texts[j][\"prompt\"]}')\n",
    "    print(f'PREDICTION: {transformer_sampling_texts[j][\"prediction\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94a3c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM жадная генерация:\n",
      "PROMT: - awww, that's a bummer. you shoulda g\n",
      "PREDICTION: - awww, that's a bummer. you shoulda gd you!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "PROMT: is upset that he can't update his facebook by texting \n",
      "PREDICTION: is upset that he can't update his facebook by texting !!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "PROMT: i dived many times for the ball. manage\n",
      "PREDICTION: i dived many times for the ball. manage to get a new one.!!!!!!!!!!!!!!\n",
      "\n",
      "PROMT: my whole body feels itc\n",
      "PREDICTION: my whole body feels itc!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "PROMT: no, it's not behaving at all. i'm mad. why am \n",
      "PREDICTION: no, it's not behaving at all. i'm mad. why am ive been up since 4am and i'm so tired!!!!!!!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('LSTM жадная генерация:')\n",
    "for i in range(5):\n",
    "    print(f'PROMT: {lstm_greedy_texts[i][\"prompt\"]}')\n",
    "    print(f'PREDICTION: {lstm_greedy_texts[i][\"prediction\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9db0e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMER жадная генерация:\n",
      "PROMT: - awww, that's a bummer. you shoulda g\n",
      "PREDICTION: osh, but I'm not going to be able to do that. I'm going to be able\n",
      "\n",
      "PROMT: is upset that he can't update his facebook by texting i\n",
      "PREDICTION: Message.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROMT: i dived many times for the ball. manage\n",
      "PREDICTION:  to get the ball out of the box.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROMT: my whole body feels itc\n",
      "PREDICTION: oughing.”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROMT: no, it's not behaving at all. i'm mad. why am \n",
      "PREDICTION: ive seen this?\n",
      "\n",
      "\n",
      "I'm not sure if it's a good idea to just say\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('TRANSFORMER жадная генерация:')\n",
    "for i in range(5):\n",
    "    print(f'PROMT: {transformer_greedy_texts[i][\"prompt\"]}')\n",
    "    print(f'PREDICTION: {transformer_greedy_texts[i][\"prediction\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec15a27",
   "metadata": {},
   "source": [
    "# Что лучше использовать\n",
    "\n",
    "Судя по метрикам, обученная модель LSTM лучше справляется задачей как на жадной генерации, так и при семплировании. Если посмотреть на сами сгенерованные тексты, то трансформер может выдавать такую девиацию как множество последовательных переносов строк, что совсем не характерно для твитов. В целом, обе модели оставляют желать лучшего. Чтобы эксперимент был честным, стоило бы дообучить трансформер\n",
    "\n",
    "Вывод: для автогенерации твитов текущая обученная LSTM работает немного лучше чем трансформер distilbert (без дообучения). Видимо, трансформер был обучен на совершенно других текстах."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp2proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
